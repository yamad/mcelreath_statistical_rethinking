# Statistical Rethinking, Chapter 4 Linear Models

## Why Gaussian distributions are 'normal'

Adding many fluctuations results in a normal:

```{r 4.1}
pos <- replicate(1000, sum(runif(16, -1, 1)))
dens(pos, col='royalblue2')
curve(dnorm(x, sd=sd(pos)), col='gray', add=T)
```

Multiplying small numbers (e.g. interactions between small effects), also converge to normal:

```{r 4.2}
prod(1 + runif(12,0,0.1))
```

```{r 4.3}
growth <- replicate(10000, prod(1 + runif(12,0,0.1)))
dens(growth, norm.comp=TRUE, col='royalblue2')
```

smaller effects get a better addititive approximation

```{r 4.4}
big <- replicate(10000, prod(1 + runif(12,0,0.5)))
small <- replicate(10000, prod(1 + runif(12,0,0.01)))

par(mfrow=c(1,3))
dens(big)
title('big')
dens(small)
title('small')
```

taking log of big multiplicative effects results in normal also:

```{r 4.5}
log.big <- replicate(10000, log(prod(1 + runif(12,0,0.5))))

dens(log.big)
title('log big')
par(mfrow=c(1,1))
```

"Repeatedly adding finite fluctuations results in a distribution of sums that have shed all information about the underlying process"

"Gaussian distributions cannot reliably identify micro-process"

"represents a particular state of ignorance": "all we are willing to say is a mean and finite variance, then Gaussian is most consistent"


## 4.2 Describing models

  1) Choose _outcome_ variable(s).

  2) Define _likelihood distribution_ for each outcome variable: the
  plausibility of an individual observation of the outcome.

  3) Choose _predictor_ variable(s) to use to predict or understand
  outcome.

  4) Relate the shape of the likelihood to the predictors by naming
  and defining all _model parameters_.

  5) Choose _priors_ for all parameters to define initial information.


## Howell (!Kung San anthropological data)

### Gaussian model of height

$$
\begin{align}
h_i &\sim \text{Normal}(\mu, \sigma)  \tag{likelihood}\\
\mu &\sim \text{Normal}(178, 20) \tag{$\mu$ prior}\\
\sigma &\sim \text{Normal}(0, 50) \tag{$\sigma$ prior}\\
\end{align}
$$

Model expects height will be a Gaussian distribution with a mean falling between $178 \pm 40$ cm with $95%$ probability, and a standard deviation of $50$ cm. That is, $95%$ of individual heights will be within $100$ cm of the average.

```{r setup-model-4.7}
library(rethinking)
data(Howell1)
d <- Howell1
```

```{r 4.10}
## filter out children (for now)
d2 <- d[ d$age >= 18 , ]
```

```{r 4.11}
par(mfrow=c(1,2))
## display prior for mu
curve( dnorm( x , 178 , 20 ) , from=100 , to=250)
```

```{r 4.12}
## display prior for sigma
curve( dunif( x , 0 , 50 ) , from=-10 , to=60)
par(mfrow=c(1,1))
```

```{r 4.13}
## sample from joint prior
sample_mu <- rnorm( 1e4 , 178 , 20 )
sample_sigma <- runif( 1e4 , 0 , 50 )
prior_h <- rnorm( 1e4 , sample_mu , sample_sigma )
dens( prior_h )
```

## Grid approximation


```{r 4.14}
## calculate posterior using grid approx
mu.list <- seq(from=140, to=160, length.out=200)
sigma.list <- seq(from=4 , to=9, length.out=200)
post <- expand.grid(mu=mu.list, sigma=sigma.list)
post$LL <- sapply(1:nrow(post),
                  function(i) sum(dnorm(
                                d2$height,
                                mean=post$mu[i],
                                sd=post$sigma[i],
                                log=TRUE)))
post$prod <- post$LL + dnorm(post$mu, 178, 20, TRUE) +
  dunif(post$sigma, 0, 50, TRUE)
post$prob <- exp(post$prod - max(post$prod))
```

```{r 4.15}
## display
par(mfrow=c(1,2))
contour_xyz(post$mu, post$sigma, post$prob)
```

```{r 4.16}
image_xyz(post$mu, post$sigma, post$prob)
```

```{r 4.17}
## sample rows from the posterior, in proportion to probability
sample.rows <- sample(1:nrow(post), size=1e4, replace=TRUE,
                      prob=post$prob)
sample.mu <- post$mu[sample.rows]
sample.sigma <- post$sigma[sample.rows]
```

```{r 4.18}
## examine joint posterior
par(mfrow=c(1,1))
plot(sample.mu, sample.sigma, cex=0.5, pch=16,
     col=col.alpha(rangi2,0.1))
```

```{r 4.19}
## examine marginals
par(mfrow=c(1,2))
dens(sample.mu)
dens(sample.sigma)
par(mfrow=c(1,1))
```

```{r 4.20}
## summarize with highest posterior density interval
HPDI(sample.mu)
HPDI(sample.sigma)
```

### Sigma will not necessarily be Gaussian

```{r 4.21}
d3 <- sample(d2$height, size=20)
```

```{r 4.22}
mu.list <- seq( from=150, to=170 , length.out=200 )
sigma.list <- seq( from=4 , to=20 , length.out=200 )
post2 <- expand.grid( mu=mu.list , sigma=sigma.list )
post2$LL <- sapply( 1:nrow(post2) , function(i)
    sum( dnorm( d3 , mean=post2$mu[i] , sd=post2$sigma[i] ,
    log=TRUE ) ) )
post2$prod <- post2$LL + dnorm( post2$mu , 178 , 20 , TRUE ) +
    dunif( post2$sigma , 0 , 50 , TRUE )
post2$prob <- exp( post2$prod - max(post2$prod) )
sample2.rows <- sample( 1:nrow(post2) , size=1e4 , replace=TRUE ,
    prob=post2$prob )
sample2.mu <- post2$mu[ sample2.rows ]
sample2.sigma <- post2$sigma[ sample2.rows ]
plot( sample2.mu , sample2.sigma , cex=0.5 ,
    col=col.alpha(rangi2,0.1) ,
    xlab="mu" , ylab="sigma" , pch=16 )
```

```{r 4.23}
## R code 4.23
dens( sample2.sigma , norm.comp=TRUE )
```

# Fit the model with MAP (quadratic approximation)

`mu` and `sigma` are parameters of the model.

```{r 4.24}
library(rethinking)
data(Howell1)
d <- Howell1
d2 <- d[d$age >= 18,]
```

```{r 4.25}
# map model
flist <- alist(
  height ~ dnorm(mu, sigma),            # Gaussian likelihood
  mu ~ dnorm(178, 20),                  # mu prior
  sigma ~ dunif(0, 50)                  # sigma prior
)
```

```{r 4.26}
m4.1 <- map(flist, data=d2)             # fit model flist to data d2
```

```{r 4.27}
precis(m4.1)                            # summary
```

A place to play with priors:


```{r 4.29}
m4.2 <- map(
  alist(
    height ~ dnorm(mu, sigma),
    mu ~ dnorm(178, 0.1),
    sigma ~ dunif(0, 50)
  ),
  data=d2)
precis(m4.2)
```

The MAP estimate makes a Gaussian approximation for each parameter of the model. The posterior then combines these into a single distribution: a multivariate Gaussian. It is described by a list of means (one for each dimension) and a _variance-covariance matrix_, which describes how each parameter in the model relates to every other parameter:

```{r 4.30}
vcov(m4.1)                              # variance-covariance matrix
```

```{r 4.31}
diag(vcov(m4.1))                        # variances for each parameter
cov2cor(vcov(m4.1))                     # correlation matrix
```

```{r 4.32}
# sampling from the posterior, a multivariate gaussian
post <- extract.samples(m4.1, n=1e4)
head(post)
```

```{r 4.33}
# summarize samples. will be close to map estimate
precis(post)
```



### Sidebar: Sampling from a multivariate Gaussian:

```{r}
# how extract.samples works
library(MASS)
post2 <- mvrnorm(n=1e4, mu=coef(m4.1), Sigma=vcov(m4.1))
```

### Sidebar: Using $\log{\sigma} as parameter

$\sigma$ often doesn't follow a Gaussian, because it cannot be negative. But $\log{\sigma}$ can be Gaussian, or close to it, because log-sigma is continuous:

```{r log-sigma}
m4.1_logsigma <- map(
  alist(
    height ~ dnorm(mu, exp(log_sigma)), # convert in likelihood
    mu ~ dnorm(178, 20),
    log_sigma ~ dnorm(2, 10)            # log_sigma has Gaussian prior
  ), data=d2)
```

```{r 4.36}
post3 <- extract.samples(m4.1_logsigma)
sigma3 <- exp(post3$log_sigma)            # convert from log units
```

$\exp(x) > 0$  for any real $x$, so it is a useful trick for constraining a parameter to be positive.

## Predictor: how do height and weight covary?

```{r 4.37}
plot(d2$height ~ d2$weight)
```

The new model:

$$
\begin{align*}
h_i &\sim \text{Normal}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta x_i \\
\alpha &\sim \text{Normal}(178, 100) \\
\beta &\sim \text{Normal}(0, 10) \\
\sigma &\sim \text{Uniform}(0, 50)
\end{align*}
$$

```{r 4.38}
m4.3 <- map(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b*weight,
    a ~ dnorm(178, 100),
    b ~ dnorm(0, 10),
    sigma ~ dunif(0, 50)
  ), data=d2)
```

With `corr`, precis shows the variance-covariance matrix:

```{r 4.41}
precis(m4.3, corr=TRUE)
```

Centering the predictor

```{r 4.42}
d2$weight.c <- d2$weight - mean(d2$weight)
```

```{r 4.43}
m4.4 <- map(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b*weight.c,
    a ~ dnorm(178, 100),
    b ~ dnorm(0, 10),
    sigma ~ dunif(0, 50)
  ), data=d2)
```

```{r 4.44}
precis(m4.4, corr=TRUE)
```

Plotting the MAP values

```{r 4.45}
# adding MAP values for mean height
plot(height ~ weight, data=d2, col="royalblue2")
abline(a=coef(m4.3)["a"], b=coef(m4.3)["b"])
```

Plotting sampled lines from posterior fit

```{r 4.46}
post <- extract.samples(m4.3)
```

Each row is a correlated random sample from joint posterior for all parameters:

```{r 4.47}
post[1:5,]
```

```{r fig-4.5}
plot_w_lines <- function(N) {
  dN <- d2[1:N, ]
  mN <- map(
    alist(
      height ~ dnorm(mu, sigma),
      mu <- a + b*weight,
      a ~ dnorm(178, 100),
      b ~ dnorm(0, 10),
      sigma ~ dunif(0, 50)
    ), data=dN)

  ## extract 20 samples from the posterior
  post <- extract.samples(mN, n=20)

  ## display raw data and sample size
  plot(dN$weight, dN$height,
       xlim=range(d2$weight), ylim=range(d2$height),
       col=rangi2, xlab="weight", ylab="height")
  mtext(concat("N = ", N))

  for (i in 1:20)
    abline(a=post$a[i], b=post$b[i], col=col.alpha("black",0.3))
}

par(mfrow=c(2,2))
sapply(c(10, 50, 150, 352), plot_w_lines)
```

Plotting an interval around a line

```{r 4.50}
## example for mu distribution for one value of weight
par(mfrow=c(1,1))
post <- extract.samples(m4.3)
mu_at_50 <- post$a + post$b * 50
```

```{r 4.51}
dens(mu_at_50, col=rangi2, lwd=2, xlab="mu|weight=50")
```

```{r 4.53}
mu <- link(m4.3)       # each column is an individual in original data
str(mu)
```

Getting distribution for mean value for each weight.

```{r 4.54}
## get 1000 plausible mu values for each weight
weight.seq <- seq(from=25, to=70, by=1)
mu <- link(m4.3, data=data.frame(weight=weight.seq))
str(mu)
```

```{r 4.55}
## plot the first 100 of these samples.
##   there is a Gaussian at each weight
par(mfrow=c(1,2))
plot(height ~ weight, d2, type="n")
for (i in 1:100)
  points(weight.seq, mu[i,], pch=16, col=col.alpha(rangi2,0.1))
```

```{r 4.56}
## summarize mu
mu.mean <- apply(mu, 2, mean)
mu.HPDI <- apply(mu, 2, HPDI, prob=0.89)
```

```{r 4.57}
plot(height ~ weight, data=d2, col=col.alpha(rangi2,0.5))
lines(weight.seq, mu.mean)              # plot MAP line
shade(mu.HPDI, weight.seq)
```

### How `link` works

```{r 4.58}
post <- extract.samples(m4.3)
mu.link <- function(weight) post$a + post$b * weight
weight.seq <- seq(from=25, to=70, by=1)
mu <- sapply(weight.seq, mu.link)
mu.mean <- apply(mu, 2, mean)
mu.HPDI <- apply(mu, 2, HPDI, prob=0.89)
```

## Prediction Intervals

So far, have only incorporated uncertainty in mu, but not the uncertainty in predicting height from mu. This latter uncertainty is summarized by the standard deviation `sigma` for height given mu.

```{r 4.59}
## simulate heights
sim.height <- sim(m4.3, data=list(weight=weight.seq))
str(sim.height)
```

```{r 4.60}
## summarize prediction interval
height.PI <- apply(sim.height, 2, PI, prob=0.89)
```

```{r 4.61}
# plot raw data
plot(height ~ weight, d2, col=col.alpha(rangi2, 0.5))
# draw MAP line
lines(weight.seq, mu.mean)
# draw HPDI region for line
shade(mu.HPDI, weight.seq)
# draw PI region for simulated heights
shade(height.PI, weight.seq)
```

```{r 4.62}
# more samples for smoother extremes
sim.height <- sim(m4.3, data=list(weight=weight.seq), n=1e4)
height.PI <- apply(sim.height, 2, PI, prob=0.89)
```

#### Rolling your own `sim`

```{r 4.63}
post <- extract.samples(m4.3)
weight.seq <- 25:70
sim.height <- sapply(weight.seq, function(weight)
  rnorm(
    n=nrow(post),
    mean=post$a + post$b * weight,
    sd=post$sigma
  )
)
height.PI <- apply(sim.height, 2, PI, prob=0.89)
```

# Polynomial regression

```{r 4.64}
library(rethinking)
data(Howell1)
d <- Howell1
str(d)
```

```{r 4.65}
d$weight.s <- (d$weight - mean(d$weight)) / sd(d$weight)
```

```{r 4.66}
d$weight.s2 <- d$weight.s^2
m4.5 <- map(
    alist(
      height ~ dnorm(mu, sigma),
      mu <- a + b1*weight.s + b2*weight.s2,
      a ~ dnorm(178, 100),
      b1 ~ dnorm(0, 10),
      b2 ~ dnorm(0, 10),
      sigma ~ dunif(0, 50)
    ),
  data=d)
```

```{r 4.67}
precis(m4.5)
```

```{r 4.68}
par(mfrow=c(1,2))
weight.seq <- seq(from=-2.2, to=2, length.out=30)
pred_dat <- list(weight.s=weight.seq, weight.s2=weight.seq^2)
mu <- link(m4.5, data=pred_dat)
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI, prob=0.89)
sim.height <- sim(m4.5, data=pred_dat)
height.PI <- apply(sim.height, 2, PI, prob=0.89)
```

```{r 4.69}
plot(height ~ weight.s, d, col=col.alpha(rangi2,0.5))
lines(weight.seq, mu.mean)
shade(mu.PI, weight.seq)
shade(height.PI, weight.seq)
```

```{r 4.70}
d$weight.s3 <- d$weight.s^3
m4.6 <- map(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b1*weight.s + b2*weight.s2 + b3*weight.s3,
    a ~ dnorm(178, 100),
    b1 ~ dnorm(0, 10),
    b2 ~ dnorm(0, 10),
    b3 ~ dnorm(0, 10),
    sigma ~ dunif(0, 50)
  ),
  data=d)
```

```{r}
weight.seq <- seq(from=-2.2, to=2, length.out=30)
pred_dat <- list(weight.s=weight.seq, weight.s2=weight.seq^2, weight.s3=weight.seq^3)
mu <- link(m4.6, data=pred_dat)
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI, prob=0.89)
sim.height <- sim(m4.6, data=pred_dat)
height.PI <- apply(sim.height, 2, PI, prob=0.89)

plot(height ~ weight.s, d, col=col.alpha(rangi2,0.5))
lines(weight.seq, mu.mean)
shade(mu.PI, weight.seq)
shade(height.PI, weight.seq)
```

## Exercises

### Easy

#### 4E1

$y_i \sim \text{Normal}(\mu, \sigma)$ is the likelihood

#### 4E2

There are two parameters in the posterior, $\mu$ and $\sigma$

#### 4E3

$$Pr(\mu, \sigma | y ) = \frac
{\mult_i \text{Normal}(y_i | \mu, \sigma) \text{Normal}(\mu|0, 10) \text{Uniform}(\sigma | 0, 10)}{\iint \mult_i \text{Normal}(y_i | \mu, \sigma) \text{Normal}(\mu|0, 10) \text{Uniform}(\sigma | 0, 10) \,d\mu \,d\sigma}$$

#### 4E4

$\mu_i = \alpha + \beta x_i$ is the linear model

#### 4E5

There are 3 parameters: $\alpha$, $\beta$, and $\sigma$


### Medium

#### 4M1

```{r ex-4M1}
prior_mu <- rnorm(1e4, 0, 10)
prior_sigma <- runif(1e4, 0, 10)
prior_y <- rnorm(1e4, mean=prior_mu, sd=prior_sigma)
dens(prior_y)
```

#### 4M2

```{r ex-4M2}
alist(
    height ~ dnorm(mu, sigma),
    mu ~ dnorm(0, 10),
    sigma ~ dunif(0, 10)
    )
```

#### 4M3

$$
\begin{align}
y_i &\sim \text{Normal}(\mu, \sigma) \\
mu_i &= \alpha + \beta*x_i \\
\alpha &\sim \text{Normal}(0, 50) \\
\beta &\sim \text{Uniform}(0, 10) \\
\sigma &\sim \text{Uniform}(0, 50)
\end{align}
$$

#### 4M4

$$
\begin{align}
height &\sim \text{Normal}(\mu, \exp(\log(\sigma))) \\
mu_i &= \alpha + \beta*\text{year}__i \\
\alpha &\sim \text{Normal}(120, 60) \\
\beta &\sim \text{Normal}(4, 10) \\
\log(\sigma) &\sim \text{Normal}(3, 10)
\end{align}
$$

  - prior for $\alpha$ is centered around 120 cm (~ 4 ft), but has a fairly broad variance that accepts that the initial heights may easily be as little as 2 ft to 6 ft.
  - $\beta$ prior suggests that students may grow roughly 2 inches a year, but could just as well grow not at all or about half a foot.
  - $\sigma$ prior is log transformed to constrain $\sigma > 0$. On the log scale, the prior parameters suggest that a best guess is that $\sigma$ is $\exp(3) \approx 20 cm$, but could be almost anything ($\exp(3-10) \approx 0, \exp(13) \approx 450000). This is probably weaker than necessary.

#### 4M5

Now that we _know_ $\alpha$ is 120 cm, the variance of the $\alpha$ prior can be 0. One option would be to remove it as a parameter, but perhaps better to give a very tight prior around 120 cm. Say, $\alpha \sim \text{Normal}(120, 2).

All students got taller suggests a prior that must be positive. Perhaps an exponential prior is a better fit, $\beta \sim \text{Exp}(...)$, or a log-transformed prior $\log(\beta) \sim \text{Normal}(1, 1.2).

```{r}
par(mfcol=c(1,2))
curve(dnorm(x, 120, 2), xlim=c(100, 140), n=1e4)
curve(dnorm(log(x), 1, 1.2), xlim=c(0, 100), n=1e4)
```

#### 4M6

A height variance ($\sigma^2$) always under 64 cm allows a narrower
$\sigma$ prior. We used a log-transformed prior, and $\ln(\sqrt(64)) = 2$, so a new prior might be $\log(\sigma) \sim \text{Normal(1, .4)}$. This puts very little mass above 64 cm.

```{r}
par(mfcol=c(1,1))
curve(dnorm(log(sqrt(x)), 1, .4), xlim=c(0, 100), n=1e4)

1 - pnorm(log(sqrt(64)), 1, .4) # 0.3%
```

This leaves the updated final model as

$$
\begin{align}
height &\sim \text{Normal}(\mu, \sigma) \\
mu_i &= \alpha + \beta*\text{year}__i \\
\alpha &\sim \text{Normal}(120, 2) \\
\log(beta) &\sim \text{Normal}(1, 1.2) \\
\log(\sigma) &\sim \text{Normal}(1, 0.4)
\end{align}
$$

### Hard

#### 4H1

```{r ex-4H1}
df <- data.frame(weight=c(46.95, 43.72, 64.78, 32.59, 54.63))
pred <- link(m4.3, data=df)

df$pred <- apply(pred, 2, mean)
df[c('hpdi_89lo', 'hpdi_89hi')] <- t(apply(pred, 2, HPDI, prob=0.89))

plot(seq(nrow(df)), df$pred, type='p', pch=20)
sapply(seq(nrow(df)), function(row) {
  lines(c(row, row), df[row,c('hpdi_89lo', 'hpdi_89hi')]) })
```

#### 4H2

```{r ex-4M2}
d18 <- Howell1[Howell1$age < 18,]
m4.7 <- map(
  alist(
    height ~ dnorm(mu, exp(log_sigma)),
    mu <- a + b*weight,
    a ~ dnorm(140, 100),
    b ~ dnorm(0, 10),
    log_sigma ~ dnorm(1, 2)
  ), data=d18)

precis(m4.7)
plot(precis(m4.7))
```

The model predicts a 27.2 cm +/- 0.07 increase in height for every 10 units increase in weight.


```{r}
pred_data <- data.frame(weight=seq(0, 50, length.out=1e3))
mu <- link(m4.7, data=pred_data)
mu.mean <- apply(mu, 2, mean)
mu.HPDI <- apply(mu, 2, HPDI, prob=0.89)
sim.height <- sim(m4.7, data=pred_data)
height.HPDI <- apply(sim.height, 2, HPDI, prob=0.89)

plot(height ~ weight, data=d18, col=col.alpha(rangi2,0.5))
lines(pred_data$weight, mu.mean)
shade(mu.HPDI, pred_data$weight)
shade(height.HPDI, pred_data$weight)
```

(c) The relationship is clearly not linear across this age range, so the linear fit assumption is way off in this case. A better fit might be obtained with a 2nd-order polynomial fit, although that would be poorly motivated theoretically. In this case, we have good theory-driven reasons for another model: we should probably include age as an interaction.


#### 4H3

```{r ex-4H3}
m4.8 <- map(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b * log(weight),
    a ~ dnorm(178, 100),
    b ~ dnorm(0, 100),
    sigma ~ dunif(0, 50)
  ), data=Howell1)

precis(m4.8)
```

Coefficient interpretation is tricky in this case. `coef(a)` is when log(weight) is 0, so when weight is `e` kg. The value is negative at this value, so it isn't easily interpretable or meaningful.

`coef(b)` is the change in height predicted when log-weight changes by 1. That is, `coef(b)` corresponds to a change in height for every e-fold change in weight.

`sigma` is an error measure of the amount of uncertainty in the estimate of height from log-weight.

```{r}
plot(height ~ weight, data=Howell1,
     col=col.alpha(rangi2, 0.4))

pred_data <- data.frame(weight=seq(0, 50, length.out=1e3))
mu <- link(m4.8, data=pred_data)
mu.mean <- apply(mu, 2, mean)
mu.HPDI <- apply(mu, 2, HPDI, prob=0.97)

sim.height <- sim(m4.8, data=pred_data)
height.HPDI <- apply(sim.height, 2, HPDI, prob=0.97)

plot(height ~ weight, data=d18, col=col.alpha(rangi2,0.5))
lines(pred_data$weight, mu.mean)
shade(mu.HPDI, pred_data$weight)
shade(height.HPDI, pred_data$weight)
```
