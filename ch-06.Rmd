---
output:
  pdf_document: default
  html_document: default
---
```{r echo=FALSE}
library(rethinking)
library(tidyverse)
```

```{r 6.1}
sppnames <- c("afarensis", "africanus", "habilis", "boisei",
              "rudolfensis", "ergaster", "sapiens")
brainvolcc <- c(438, 452, 612, 521, 752, 871, 1350)
masskg <- c(37.0, 35.5, 34.5, 41.5, 55.5, 61.0, 53.5)
d <- data.frame(species=sppnames, brain=brainvolcc, mass=masskg)
```

```{r 6.2}
m6.1 <- lm(brain ~ mass, data=d)
```

```{r 6.3}
1 - var(resid(m6.1))/var(d$brain)
```

```{r 6.4}
m6.2 <- lm(brain ~ mass + I(mass^2), data=d)
```

```{r 6.5}
m6.3 <- lm(brain ~ mass + I(mass^2) + I(mass^3), data=d)
m6.4 <- lm(brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4), data=d)
m6.5 <- lm(brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) +
           I(mass^5), data=d)
m6.6 <- lm(brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) +
           I(mass^5) + I(mass^6), data=d)
```

```{r fig-6.3, echo=FALSE}
mass.seq <- seq(30, 65, length.out=50)
r2 <- function(fit) { 1 - var(resid(fit)) / var(d$brain) }

par(mfrow=c(3,2))
for (fit in list(m6.1, m6.2, m6.3, m6.4, m6.5, m6.6)) {
  print(fit)
  p <- predict(object=fit, newdata=data.frame(mass=mass.seq),
               interval="confidence", level=0.89)
  plot(brain ~ mass, data=d, col='steelblue4',
       xlab='body mass (kg)', ylab='brain volume (cc)',
       ylim=c(350, 1500))
  mtext(paste(expression(R^2), sprintf('= %.2f', r2(fit))))
  lines(mass.seq, p[,'fit'])
  shade(t(p[,-1]), mass.seq)
}
```

```{r 6.6}
m6.7 <- lm(brain ~ 1, data=d)
```

```{r fig6.4, echo=FALSE}
plot(brain ~ mass, data=d, col='steelblue4',
     xlab='body mass (kg)', ylab='brain volume (cc)',
     ylim=c(350, 1500))

mtext(paste(expression(R^2), sprintf('= %.2f', r2(m6.7))))
p <- predict(object=m6.7, newdata=data.frame(mass=mass.seq),
             interval="confidence", level=0.89)
lines(mass.seq, p[,'fit'])
shade(t(p[,-1]), mass.seq)
```

```{r fig6.5, echo=FALSE}
par(mfrow=c(1, 2))
mass.seq <- seq(30, 65, length.out=100)

plot(brain ~ mass, data=d, col='steelblue4', ylim=c(350, 1500))
for (i in 1:nrow(d)) {
  d.new <- d[-i,]
  m0 <- lm(brain ~ mass, d.new)
  lines(mass.seq, predict(m0, data.frame(mass=mass.seq)), col=col.alpha("black", 0.5))
}

plot(brain ~ mass, data=d, col='steelblue4', ylim=c(-200, 2200))
for (i in 1:nrow(d)) {
  d.new <- d[-i,]
  m0 <- lm(brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) + I(mass^5), d.new)
  lines(mass.seq, predict(m0, data.frame(mass=mass.seq)), col=col.alpha("black", 0.5))
}

par(mfrow=c(1,1))
```

### Information Theory

information
:    the reduction in uncertainty derived from learning an outcome

information entropy (uncertainty)
:    the uncertainty contained in a probability distribution is the
     average log-probability of an event.

$$
H(p) = -\mathrm{E}\,\log(p_i) = - \sum_{i=1}^n{p_i \log(p_i)}
$$

```{r 6.9}
p <- c(0.3, 0.7)
-sum(p * log(p))
```

Kullback-Leibler divergence
:    additional uncertainty induced by using probabilities from one
     distribution to describe another distribution

:    average difference in log probability between the target ($p$) and model ($q$)

$$
D_{\mathrm{KL}}(p, q) = \sum_i{ p_i \left( \log{(p_i)} - \log{(q_i)} \right) } = \sum_i { p_i \log{\left(\frac{p_i}{q_i} \right) } }
$$


### Model comparison

```{r 6.21}
data(milk)
d <- milk[complete.cases(milk),]
d$neocortex <- d$neocortex.perc / 100
dim(d)
```

```{r 6.22}
a.start <- mean(d$kcal.per.g)
sigma.start <- log(sd(d$kcal.per.g))
m6.11 <- rethinking::map(
  alist(
    kcal.per.g ~ dnorm(a, exp(log.sigma))
  ),
  data=d, start=list(a=a.start,
                     log.sigma=sigma.start))

m6.12 <- rethinking::map(
  alist(
    kcal.per.g ~ dnorm(mu, exp(log.sigma)),
    mu <- a + bn*neocortex
  ),
  data=d, start=list(a=a.start,
                     bn=0,
                     log.sigma=sigma.start))

m6.13 <- rethinking::map(
  alist(
    kcal.per.g ~ dnorm(mu, exp(log.sigma)),
    mu <- a + bm*log(mass)
  ),
  data=d, start=list(a=a.start,
                     bm=0,
                     log.sigma=sigma.start))

m6.14 <- rethinking::map(
  alist(
    kcal.per.g ~ dnorm(mu, exp(log.sigma)),
    mu <- a + bn*neocortex + bm*log(mass)
  ),
  data=d, start=list(a=a.start,
                     bn=0,
                     bm=0,
                     log.sigma=sigma.start))
```

```{r 6.23}
w <- WAIC(m6.14)
w

## calc WAIC
-2 * (attr(w, 'lppd') - attr(w, 'pWAIC'))
```

```{r 6.24}
milk.models <- rethinking::compare(m6.11, m6.12, m6.13, m6.14)
```

```{r 6.25}
plot(milk.models, SE=TRUE, dSE=TRUE)
```

```{r 6.27}
coeftab(m6.11, m6.12, m6.13, m6.14)
```

```{r 6.28}
plot(coeftab(m6.11, m6.12, m6.13, m6.14))
```

# Exercises

## Easy

### 6E1

Information entropy

1) entropy is continuous. small changes in the probability of the
outcomes lead to small changes in uncertainty. uncertainty varies
smoothly, with no jumps.

2) with more possible outcomes/messages, entropy/uncertainty is higher
(and more is learned when one outcome is chosen).

3) the uncertainties of two independent events happening separately is
equal to the uncertainty of both events happening together.

### 6E2

```{r}
events <- c('heads'=0.7, 'tails'=0.3)
-sum(events * log(events))
```

### 6E3

```{r}
events <- c(0.2, 0.25, 0.25, 0.30)
names(events) <- 1:4
-sum(events * log(events))
```

### 6E4

```{r}
events <- c(1/3, 1/3, 1/3)
names(events) <- 1:3
-sum(events * log(events))
```

## Medium

### 6M1

#### Akaike Information Criterion (AIC)

$$
\mathrm{AIC} = D_{train} + 2p
$$

for number of parameters $p$ and in-sample deviance $D_{train}$.

Recall, Deviance $D(q)$ of probability distribution $q$ is defined by

$$
D(q) = -2 \sum_{i}{\log{(q_i)}}
$$

Assumptions:

  1) flat priors
  2) Gaussian posterior
  3) sample size $N >> k$ parameters

#### DIC (Deviance Information Criterion)

$$
DIC = \bar{D} + (\bar{D} - \hat{D}) = \bar{D} + p_D
$$

for average posterior deviance $\bar{D}$ and posterior mean deviance
$\hat{D}$. Notice, $p_D = $\bar{D} - \hat{D}$ and represents an
_effective number of parameters$.

  1) _non-flat priors_
  2) Gaussian posterior
  3) $N >> k$

#### WAIC (Widely Applicable Information Criterion)

  1) _non-flat priors_
  2) _non-Gaussian posterior_
  3) calculated pointwise, at each observation
  4) not meaningful if observations are not independent

$$
WAIC = -2(\mathrm{lppd} - p_{WAIC})
$$

log-posterior predictive density:

$$
lppd = \sum_{i=1}^{N}{\log{\mathrm{Pr}(y_i)}}
$$


$$
p_{WAIC} = \sum_{i=1}^{N}{V(y_i)}
$$


#### Summary

WAIC is the most general in the sense that there are no assumptions
about the shape of the probability distributions, either for the prior
or posterior. Assuming a Gaussian posterior leads to DIC, and
additionally assuming Gaussian priors leads to AIC.

### 6M2

Model selection chooses one model, considered "best" by some criterion (possibly information criteria like WAIC). Model selection discards information about relative accuracy between models. This information can be used to evaluate confidence in the model structure.


Model averaging uses all models in each prediction, in a sort of weighted averaging of each individual model's prediction. It incorporates relative accuracy between models into one posterior predictive distribution.

There will be higher uncertainty from model averaging about the model itself and model predictions than a single model will. The estimates are conditional on the set of models considered. If many bad models are included, then uncertainty could be inflated even if a very good true model is among the set of models.


### 6M3

IC and deviance values will (almost?) always be lower with fewer observations. The model "has been asked to predict less". Note from the calculation of deviance: it is not an average, it is a sum and thus is sensitive to changes in sample size.

### 6M4

Regularization constrains model flexibility which is reflected as a lower number of effective parameters as the regularizing prior gets stronger (more concentrated).

### 6M5

An informative prior is like adding many samples from previously collected data, thus reducing the influence of the sampling peculiarities of the current data.

### 6M6

Overly informative priors don't incorporate new evidence easily, because the belief is very strong and concentrated already. New evidence that falls outside of a narrow region is given very low probability.


## Hard

```{r}
library(rethinking)
data(Howell1)
d <- Howell1
d$age <- (d$age - mean(d$age)) / sd(d$age)
set.seed(1000)
i <- sample(1:nrow(d), size=nrow(d)/2)
d1 <- d[i,]
d2 <- d[-i,]
```

```{r}
height.start <- mean(d1$height)
m6h1.1 <- rethinking::map(
    alist(
        height ~ dnorm(mu, sigma),
        mu <- a + b1*age,
        a <- dnorm(0, 10),
        b1 <- dnorm(0, 10),
        sigma ~ dunif(0, 100)
    ),
    data=d1, start=list(a=height.start))
m6h1.2 <- rethinking::map(
    alist(
        height ~ dnorm(mu, sigma),
        mu <- a + b1*age + b2*age^2,
        a <- dnorm(0, 10),
        c(b1, b2) <- dnorm(0, 10),
        sigma ~ dunif(0, 100)
    ),
    data=d1, start=list(a=height.start))
m6h1.3 <- rethinking::map(
    alist(
        height ~ dnorm(mu, sigma),
        mu <- a + b1*age + b2*age^2 + b3*age^3,
        a <- dnorm(0, 10),
        c(b1, b2, b3) <- dnorm(0, 10),
        sigma ~ dunif(0, 100)
    ),
    data=d1, start=list(a=height.start))
m6h1.4 <- rethinking::map(
    alist(
        height ~ dnorm(mu, sigma),
        mu <- a + b1*age + b2*age^2 + b3*age^3 + b4*age^4,
        a <- dnorm(0, 10),
        c(b1, b2, b3, b4) <- dnorm(0, 10),
        sigma ~ dunif(0, 100)
    ),
    data=d1, start=list(a=height.start))
m6h1.5 <- rethinking::map(
    alist(
        height ~ dnorm(mu, sigma),
        mu <- a + b1*age + b2*age^2 + b3*age^3 + b4*age^4 + b5*age^5,
        a <- dnorm(0, 10),
        c(b1, b2, b3, b4, b5) <- dnorm(0, 10),
        sigma ~ dunif(0, 100)
    ),
    data=d1, start=list(a=height.start))
m6h1.6 <- rethinking::map(
    alist(
        height ~ dnorm(mu, sigma),
        mu <- a + b1*age + b2*age^2 + b3*age^3 + b4*age^4 + b5*age^5 + b6*age^6,
        a <- dnorm(0, 10),
        c(b1, b2, b3, b4, b5, b6) <- dnorm(0, 10),
        sigma ~ dunif(0, 100)
    ),
    data=d1, start=list(a=height.start))

m6h1.models <- paste0("m6h1.", 1:6)
```

### 6H1

```{r}
compare_ <- do.call(compare, sapply(m6h1.models, as.name))
compare_
```

Model 4 has the lowest WAIC value, and carries most of the weighting: 0.89.

### 6H2

```{r}
par(mfrow=c(2, 3))

x.seq <- seq(min(d1$age), max(d1$age), length.out=100)
for (mod_name in m6h1.models) {
    mod <- get(mod_name)
    mu <- link(mod, data=data.frame(age=x.seq))
    mu.mean <- apply(mu, 2, mean)
    mu.PI <- apply(mu, 2, PI, prob=0.97)
    plot(height ~ age, data=d1, col="steelblue4")
    lines(x.seq, mu.mean, type='l')
    shade(mu.PI, x.seq)
    title(mod_name)
}
```
